import numpy as np
import scipy
import scipy.stats

import matplotlib.pyplot as plt
from ipywidgets import interact

plt.style.use('fivethirtyeight')
%matplotlib inline

def normalize(X):
    """Normalize the given dataset X to have zero mean.
    Args:
        X: ndarray, dataset of shape (N,D) where D is the dimension of the data,
           and N is the number of datapoints
    
    Returns:
        (Xbar, mean): tuple of ndarray, Xbar is the normalized dataset
        with mean 0; mean is the sample mean of the dataset.
    """
    N, D = X.shape
    mu = np.mean(X, axis=0) # <-- computes the mean of X
    Xbar = X - mu            # <-- computes the normalized data Xbar by subtracting mu from each row of X
    return Xbar, mu

def eig(S):
    """Compute the eigenvalues and corresponding eigenvectors
        for the covariance matrix S.
    Args:
        S: ndarray, covariance matrix

    Returns:
        (eigvals, eigvecs): ndarray, the eigenvalues and eigenvectors

    Note:
        the eigenvals and eigenvecs should be sorted in descending
        order of the eigen values
    """

     # Compute the eigenvalues and eigenvectors
     # Note that you can compute both of these with just a single function call
    eigvals, eigvecs = np.linalg.eig(S)
  
     # The eigenvalues and eigenvectors need to be sorted in descending order according to the eigenvalues
     # We will use `np.argsort` to find a permutation of the indices of eigvals that will sort eigvals in ascending order and
     # then find the descending order via [::-1], which reverse the indices
     # (https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html)
    sort_indices = np.argsort(eigvals)[::-1]
  
     # Notice that we are sorting the columns (not rows) of eigvecs since the columns represent the eigenvectors.
    return eigvals[sort_indices], eigvecs[:, sort_indices]

def _flip_eigenvectors(B):
    """Flip the eigenvectors.    
    """
    signs = np.sign(B[np.argmax(np.abs(B), axis=0), range(B.shape[1])])
    return B * signs

def _normalize_eigenvectors(B):
    # Normalize eigenvectors to have unit length
    # Also flip the direction of the eigenvector based on
    # the first element
    B_normalized = B / np.linalg.norm(B, axis=0)
    for i in range(B.shape[1]):
        if (B_normalized[0, i] < 0):
            B_normalized[:, i] = -B_normalized[:, i]
    return B_normalized

def projection_matrix(B):
    """Compute the projection matrix onto the space spanned by `B`
    Args:
        B: ndarray of dimension (D, M), the basis for the subspace
    
    Returns:
        P: the projection matrix
    """

    return B @ np.linalg.inv(B.T @ B) @ B.T # <-- EDIT THIS to compute the projection matrix

def PCA(X, num_components):
    
    """
    Args:
        X: ndarray of size (N, D), where D is the dimension of the data,
           and N is the number of datapoints
        num_components: the number of principal components to use.
    Returns:
        the reconstructed data, the sample mean of the X, principal values
        and principal components
    """

    N, D = X.shape
    # first perform normalization on the digits so that they have zero mean and unit variance
    X_normalized, mean = normalize(X)
    # Then compute the data covariance matrix S
    S = 1/N*(X_normalized.T @ X_normalized)

    # Next find eigenvalues and corresponding eigenvectors for S
    eig_vals, eig_vecs = eig(S)
    # Take the top `num_components` of eig_vals and eig_vecs,
    # This will be the corresponding principal values and components
    # Remember that the eigenvectors are the columns of the matrix `eig_vecs`
    principal_vals, principal_components = eig_vals[:num_components], eig_vecs[:, :num_components]

    # Due to precision errors, the eigenvectors might come out to be complex, so only take their real parts
    principal_components = np.real(principal_components) 

    # Reconstruct the data by projecting the normalized data on the basis spanned by the principal components
    # Remember that the data points in X_normalized are arranged along the rows
    # but while projecting, we need them to be arranged along the columns
    # Notice that we have subtracted the mean from X so make sure that you add it back
    # to the reconstructed data

    P = projection_matrix(principal_components)
    reconst = (P @ X_normalized.T).T + mean
    return reconst, mean, principal_vals, principal_components
